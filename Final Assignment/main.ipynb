{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle\n",
    "\n",
    "Group 1\n",
    "\n",
    "Jenewein Matthias - Jenewein Matthias\n",
    "\n",
    "Kalarickal Dominic - Kalarickal Dominic\n",
    "\n",
    "Leander Leirissa - Bitterzoet\n",
    "\n",
    "Timmer Lars - laltir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Loading packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If not all libraries are installed, uncomment the cell below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import functions as f\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, NMF\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import os\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Loading labeled data labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv('Datasets/labels_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_files = os.listdir('Datasets/labeled')\n",
    "unlabeled_files = os.listdir('Datasets/unlabeled')\n",
    "\n",
    "print(\"Labeled files:\", labeled_files)\n",
    "print(\"Unlabeled files:\", unlabeled_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and Process Features from Audio Files  \n",
    "This cell handles the loading and processing of features extracted from labeled and unlabeled audio files:  \n",
    "- An instance of the `DataLoader` class is initialized.  \n",
    "- The `featureDataFrame` method is called twice to generate DataFrames containing features for labeled and unlabeled files:  \n",
    "  - Labeled features are extracted from the `labeled_files` directory and saved in `labeled_features_df`.  \n",
    "  - Unlabeled features are extracted from the `unlabeled_files` directory and saved in `unlabeled_features_df`.  \n",
    "- The `labeled_features_df` DataFrame is merged with the `labels` DataFrame on the `filename` column to associate labels with the features.  \n",
    "\n",
    "Finally, both the labeled and unlabeled DataFrames are displayed to preview their structure and contents.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl = f.DataLoader()\n",
    "\n",
    "labeled_features_df = dl.featureDataFrame(labeled_files, 'Datasets/labeled')\n",
    "unlabeled_features_df = dl.featureDataFrame(unlabeled_files, 'Datasets/unlabeled')\n",
    "\n",
    "labeled_features_df = labeled_features_df.merge(labels, on='filename')\n",
    "\n",
    "print(\"Labeled Features DataFrame\")\n",
    "display(labeled_features_df.head())\n",
    "\n",
    "print(\"\\nUnlabeled Features DataFrame\")\n",
    "display(unlabeled_features_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Explanations and Calculations in Machine Learning Audio Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract different features from the audio files, we used the `librosa` library. The librosa library offers many different features to explore and is therefore particularly useful for a task like this.\n",
    "\n",
    "For a reference please, have a look at the function.py or more specifically the \"extract_features\" method as well as the librosa documentation (https://librosa.org/doc/latest/index.html). All the methods used were taken from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Spectral Centroid\n",
    "\n",
    "**Explanation:**  \n",
    "The spectral centroid represents the center of gravity of the spectral energy distribution. A higher spectral centroid is indicative of a brighter sound. It quantifies the frequency where the majority of the signal's energy resides (Sable, 2021).  \n",
    "\n",
    "**Calculation:**  \n",
    "Using the `librosa` library, the spectral centroid is computed as the weighted mean of the frequencies, with the magnitudes serving as weights (Librosa, n.d.-a).  \n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "\\text{Spectral Centroid}[t] = \\frac{\\sum_{k} S[k, t] \\cdot \\text{freq}[k]}{\\sum_{j} S[j, t]}\n",
    "$$\n",
    "Where:  \n",
    "- \\(s\\): Magnitude spectogram  \n",
    "- \\(freq\\): Array of frequency values. \n",
    "\n",
    "(Librosa, n.d.-a).\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the spectral centroid differs vastly between genres. -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('spectral_centroid', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spectral Bandwidth\n",
    "\n",
    "**Explanation:**  \n",
    "Spectral bandwidth represents the spread (distinction between high and low) frequencies. The bandwidth indicates a how noisy or pure a sound sounds (Jakeli, 2023).  \n",
    "\n",
    "**Calculation:**  \n",
    "The spectral bandwidth is computed as the weighted standard deviation of frequencies (Music Information Retrieval, n.d.)\n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "\\left( \\sum_{k} S[k, t] \\cdot \\left( \\text{freq}[k, t] - \\text{centroid}[t] \\right)^p \\right)^{\\frac{1}{p}}\n",
    "$$ \n",
    "Where:  \n",
    "- \\(x(n)\\): Magnitude of the frequency bin.  \n",
    "- \\(f(n)\\): Frequency value.  \n",
    "\n",
    "Librosa (n.d.-b)\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the spectral bandwith differs vastly between genres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('spectral_bandwidth', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Zero Crossing Rate (ZCR)\n",
    "\n",
    "**Explanation:**  \n",
    "Zero Crossing Rate measures the rate at which a signal crosses the zero amplitude line (so the prefix changes from positive to negative or vice versa) (OpenAE, n.d.-a). ZCR is an important indicator to capture the smoothness of an audio file (Bäckström et al., 2022).  \n",
    "\n",
    "**Calculation:**  \n",
    "The ZCR is computed by summing up the zero crossings in a signal and the normalizing by the amount of consecutive samples (=N) (Bäckström et al., 2022).\n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "ZCR_k = \\frac 1 N \\sum_{h=kM}^{kM+N} |\\text{sign}(x_h) - \\text{sign}(x_{h-1})|,\n",
    "$$\n",
    "\n",
    "where \\( M \\) is the step between analysis windows and \\( N \\) the analysis window length (Bäckström et al., 2022).\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the zero crossing rate differs vastly between genres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('zero_crossing_rate', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Root Mean Square (RMS)\n",
    "\n",
    "**Explanation:**  \n",
    "Root Mean Square (RMS) quantifies the loudness or energy of an audio signal. Higher RMS values correspond to louder sounds (Miraglia, 2024).  \n",
    "\n",
    "**Calculation:**  \n",
    "The RMS is computed as the square root of the mean of the squared amplitudes. (Wikipedia, 2024-a)  \n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "RMS = \\sqrt{\\frac{1}{N} \\sum_{i}^{N-1} x_i^2}\n",
    "$$\n",
    "\n",
    "(OpenAE, n.d.-b)\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the root mean square differs vastly between genres.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('rms', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Spectral Rolloff\n",
    "\n",
    "**Explanation:**  \n",
    "Spectral rolloff defines the frequency below which a specified percentage (e.g., 85%) of the total spectral energy is concentrated (OpenAE, n.d.-b). This feature has influence on the frequency of the sound (Librosa, n.d.)\n",
    "\n",
    "**Calculation:**  \n",
    "The spectral rolloff is calculated by the n% spectral roll off point which is the exact frequency that marks the specicified percentage (e.g., 85%) below the n% energy of all energy is stored (OpenAE, n.d.-c).\n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "\\sum_{m=0}^{r} X_p[m] \\geq \\frac{n}{100} \\sum_{m=0}^{M-1} X_p[m]\n",
    "$$ \n",
    "\n",
    "(OpenAE, n.d.-c)\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the spectral rolloff differs vastly between genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('spectral_rolloff', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. MFCC Means\n",
    "\n",
    "**Explanation:**  \n",
    "Mel-Frequency Cepstral Coefficients (MFCCs) characterize the tonal and textural qualities of an audio signal (perception of loudness or tempo for example). Nowadays, MFCCs are widely used to characterize sound (Sable, 2021).\n",
    "\n",
    "**Calculation:**  \n",
    "The MFCC is calculated by computing the cepstrum coefficient for each frame (Wikipedia, 2024-c). Computing the mean MFCC provides a summary of these features across an entire audio clip.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "\n",
    "$$\n",
    "c_i = \\sum_{n=1}^{N_f} S_n \\cos \\left( (n - 0.5) \\left( \\frac{i \\pi}{N_f} \\right) \\right), \\quad i = 1, \\dots, L\n",
    "$$\n",
    "\n",
    "(Wikipedia, 2024-c)\n",
    "\n",
    "The mathematical equation of calcuting the mean MFCC values:\n",
    "\n",
    "$$\n",
    "\\text{MFCC}_i = \\frac{1}{n} \\sum_{n=1}^{N} \\text{MFCC}(i, n)\n",
    "$$  \n",
    "\n",
    "(Wikipedia, 2025-b).\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the MFCC mean differs vastly between genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('mfcc_mean_1', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display not just one mean value, we can use a multi-line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature_multiline(df=labeled_features_df, feature_prefix='mfcc_mean_', num_sub_features=7, x_col='genre', x_label='Genre',y_label='Average MFCC Mean',title='MFCC Means per Genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Chroma Mean\n",
    "\n",
    "**Explanation:**  \n",
    "Chroma features capture the energy distribution across the 12 pitch classes (e.g., C, D, E, etc.) within an octave (Sable, 2022). The Chroma Mean represents the average intensity of these pitch classes over the entire audio clip.  \n",
    "\n",
    "**Calculation:**  \n",
    "To calculate \"such chroma vectors all tones of different octave of the corresponding 12 half tones are mapped into one octave. This means that for example tone ”A” is added to a value, whose sum represents a component of the chroma vector, regardless of its respective octave\" (Englmeier et al., 2023, p. 185).\n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "CV(i) = \\sum_{m=0}^{M-1} |X_{CQ}(i + 12m)|\n",
    "$$\n",
    "\n",
    "(Englmeier et al., 2023)\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the chroma mean differs vastly between genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('chroma_mean_1', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display not just one mean value, we can use a multi-line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature_multiline(df=labeled_features_df, feature_prefix='chroma_mean_', num_sub_features=7, x_col='genre', x_label='Genre',y_label='Average Chroma Mean',title='Chroma Means per Genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Tempo\n",
    "\n",
    "**Explanation:**  \n",
    "Tempo represents the speed of an audio signal, typically measured in beats per minute (BPM). It plays a crucial role in genre classification (Wikipedia, 2025).\n",
    "\n",
    "**Calculation:**  \n",
    "Librosa determines the BPM by finding a global (for the entire audio file) tempo first. This global tempo is then used to build a cost function and afterwards tries to find the best-fitting beat times. The times should present the tempo from the audio as well as possibly (Elis, 2007). \n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "\\text{Tempo (BPM)} = \\frac{60}{\\text{Average Beat Interval (seconds)}}\n",
    "$$\n",
    "\n",
    "(OmniCalculator, 2024)\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the tempp differs vastly between genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('tempo', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Spectral Contrast\n",
    "\n",
    "**Explanation:**  \n",
    "Spectral Contrast quantifies the amplitude difference between high-energy (peaks/top quartile) and low-energy (valleys/bottom quartile) regions within frequency bands (Sable, 2021). \n",
    "\n",
    "**Calculation:**  \n",
    "The Spectral contrast can be calculated by dividing the peak (point with highest energy) through the valley (point with lowest energy). The 10 x log10(rate) is used to properly convert the quotient into decibels (Yuto, 2024). The mean spectral contrast provides an overall summary of these values across all frames.\n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "\\text{Spectral Contrast} = 10 \\times \\log_{10} \\left( \\frac{\\text{Peak Value}}{\\text{Valley Value}} \\right)\n",
    "$$\n",
    "\n",
    "(Yuto, 2024)\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the spectral contrast mean differs vastly between genres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('spectral_contrast', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display not just one mean value, we can use a multi-line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature_multiline(df=labeled_features_df,feature_prefix='contrast_mean_',num_sub_features=7,x_col='genre', x_label='Genre',y_label='Average Spectral Contrast Mean', title='Spectral Contrast Means per Genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Tonnetz Mean\n",
    "\n",
    "**Explanation:**  \n",
    "Tonnetz features represent harmonic relationships between pitches, such as intervals or chords (Wikipedia, 2024-b).   \n",
    "\n",
    "**Calculation:**  \n",
    "Librosa transforms chroma features into the Tonnetz space and maps the interval like major third onto two-dimensional coordinates (Librosa, n.d-d). The Tonnetz Mean summarizes these relationships over time and computes the average value.\n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "\\text{Tonnetz Mean}_i = \\frac{1}{n} \\sum_{n=1}^{N} \\text{Tonnetz}(i, n)\n",
    "$$\n",
    "\n",
    "Unfortunately, we were not able to find a good formula for this feature. Hence, we decided to simple describe how the arithmetic mean of the observed Tonnetz values could be calculated (Wikipedia, 2025-b).\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the tonnetz mean differs vastly between genres. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('tonnetz_mean_1', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To display not just one mean value, we can use a multi-line plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature_multiline(df=labeled_features_df,feature_prefix='tonnetz_mean_', num_sub_features=6, x_col='genre', x_label='Genre',y_label='Average Tonnetz Mean',title='Tonnetz Means per Genre')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Spectral Flatness\n",
    "\n",
    "**Explanation:**  \n",
    "Spectral Flatness measures the resemblance of a sound to a pure tone. Lower values indicate purer tones, while higher values suggest noise-like signals (Wikipedia, 2024-c).  \n",
    "\n",
    "**Calculation:**  \n",
    "The spectral flatness is alculated by dividing the geometric mean to the arithmetic mean of the spectral magnitudes (Wikipedia, 2024-c).\n",
    "\n",
    "**Mathematical Formula:**  \n",
    "$$\n",
    "\\text{Flatness} = \\frac{geometric} {arithmetic} = \\frac{\\sqrt[N]{\\prod_{n=0}^{N-1} x(n)}}{\\frac{\\sum_{n=0}^{N-1} x(n)}{N}} \n",
    "= \\frac{\\exp\\left(\\frac{1}{N} \\sum_{n=0}^{N-1} \\ln x(n)\\right)}{\\frac{1}{N} \\sum_{n=0}^{N-1} x(n)}\n",
    "$$\n",
    "\n",
    "(Wikipedia, 2024-c)\n",
    "\n",
    "**Reason for including this feature:**  \n",
    "The chart below shows that this feature is important to distinguish genres as the spectral flatness differs vastly between genres. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.visualize_feature('flatness_mean', labeled_features_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Especially the genres \"pop\" & \"classical\" often seem to differ a lot from the rest of the genres which makes them potential candidates to be one of the clusters. Of course this is only based now on the labeled dataset and has to be analysed thoroughly.  So we calcualte the average mean for all features per genre and map them agains the average feature value across ALL features and display them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_features_by_genre = labeled_features_df.groupby('genre').mean().reset_index()\n",
    "\n",
    "avg_features_by_genre['overall_mean'] = avg_features_by_genre.drop('genre', axis=1).mean(axis=1)\n",
    "\n",
    "average_mean_across_genres = avg_features_by_genre.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_genre = labeled_features_df.groupby('genre').mean().reset_index()\n",
    "\n",
    "df_genre['overall_mean'] = df_genre.drop('genre', axis=1).mean(axis=1)\n",
    "\n",
    "average_mean_across_genres = df_genre['overall_mean'].mean()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "plt.scatter(df_genre['genre'], df_genre['overall_mean'], color='blue', s=100, label='Genre Overall Mean')\n",
    "\n",
    "plt.axhline(y=average_mean_across_genres, color='red', linestyle='--', linewidth=2, label='Average Mean Across Genres')\n",
    "\n",
    "plt.xlabel('Genre')\n",
    "plt.ylabel('Overall Mean')\n",
    "plt.title('Overall Genre Means\\nMapped around the Average Mean Across Genres')\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting the average values of all features per genre against the total average value across all features and genres, supports our first guess that the genres \"classical\" and \"pop\" are good candidates for potential clusters based on the labeled dataset. A third potential cluster could either be metal or hiphop because they seemingly display a decent average of the values of the other clusters (except classical and pop). \n",
    "\n",
    "Of course, this is only for the labeled data and needs more in-depth analysis for the unlabeled dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scale Numeric Features for NMF Compatibility  \n",
    "This cell processes numeric features from the labeled and unlabeled feature DataFrames to prepare them for Non-Negative Matrix Factorization (NMF):  \n",
    "- Numeric columns are extracted from both `unlabeled_features_df` and `labeled_features_df` using `select_dtypes(include=[np.number])`.  \n",
    "- If a `cluster` column exists, it is dropped from both DataFrames to ensure only relevant features are included.  \n",
    "- A `MinMaxScaler` is initialized with a range of 1 to 2. This scaling range was chosen to avoid issues with non-negative numbers encountered during NMF when using `StandardScaler` or a `MinMaxScaler` with a 0 to 1 range.  \n",
    "- The scaler is fitted on the unlabeled numeric features (`unlabeled_numeric`) and applied to transform both unlabeled and labeled numeric features.  \n",
    "\n",
    "The resulting scaled feature arrays (`unlabeled_scaled` and `labeled_scaled`) are now ready for dimensionality reduction or further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_numeric = unlabeled_features_df.select_dtypes(include=[np.number])\n",
    "labeled_numeric = labeled_features_df.select_dtypes(include=[np.number])\n",
    "\n",
    "if 'cluster' in unlabeled_numeric.columns:\n",
    "    unlabeled_numeric.drop('cluster', axis=1, inplace=True)\n",
    "    \n",
    "if 'cluster' in labeled_numeric.columns:\n",
    "    labeled_numeric.drop('cluster', axis=1, inplace=True)\n",
    "    \n",
    "scaler = MinMaxScaler((1, 2))\n",
    "\n",
    "unlabeled_scaled = scaler.fit_transform(unlabeled_numeric)\n",
    "labeled_scaled = scaler.transform(labeled_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_knn = pd.DataFrame(unlabeled_scaled, columns=unlabeled_numeric.columns)\n",
    "labeled_knn = pd.DataFrame(labeled_scaled, columns=labeled_numeric.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering: Overview and Workflow  \n",
    "\n",
    "K-Means Clustering is an **unsupervised learning algorithm** designed to partition an unlabeled dataset into distinct clusters. The parameter \\( K \\) specifies the number of clusters, e.g., $ K = 5 $ creates **five clusters**, and $ K = 10 $ forms **ten clusters**.  \n",
    "\n",
    "#### Objectives of K-Means Clustering:  \n",
    "1. Identify the optimal positions of $ K $ cluster centers (centroids).  \n",
    "2. Assign each data point to the closest cluster based on **distance metrics**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Workflow of the K-Means Algorithm  \n",
    "\n",
    "#### **Step 1: Choose the Number of Clusters ($ K $)**  \n",
    "- Determine $ K $ using methods like the **Elbow Method** or domain knowledge.  \n",
    "\n",
    "#### **Step 2: Initialize $ K $ Cluster Centers**  \n",
    "- Randomly select $ K $ data points as initial cluster centers.  \n",
    "\n",
    "#### **Step 3: Compute Distances**  \n",
    "- Calculate the distance between each data point and all cluster centers using the **Euclidean distance**:  \n",
    "  $$  \n",
    "  d = \\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}  \n",
    "  $$  \n",
    "\n",
    "#### **Step 4: Assign Data Points to Clusters**  \n",
    "- Allocate each data point to the cluster with the nearest center based on the computed distances.  \n",
    "\n",
    "#### **Step 5: Update Cluster Centers**  \n",
    "- Recalculate the cluster centers as the **mean** of the data points assigned to each cluster:  \n",
    "  $$  \n",
    "  c_j = \\frac{1}{N_j} \\sum_{i \\in C_j} x_i  \n",
    "  $$  \n",
    "  Where:  \n",
    "  - $ c_j $: New centroid of cluster $ j $.  \n",
    "  - $ N_j $: Number of points in cluster $ j $.  \n",
    "\n",
    "#### **Step 6: Repeat Until Convergence**  \n",
    "- Iterate **Steps 3 to 5** until one of the following occurs:  \n",
    "  - Cluster assignments **stabilize**.  \n",
    "  - A **maximum number of iterations** is reached.  \n",
    "  - The cluster centers become **unchanged**.  \n",
    "\n",
    "---\n",
    "\n",
    "### Key Notes:  \n",
    "- The algorithm assumes clusters are spherical and equally sized.  \n",
    "- Performance may depend on the initial choice of cluster centers, which is why multiple initializations are often used.  \n",
    "\n",
    "#### Reference  \n",
    "Dharmaraj, 2022  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hierarchical Clustering  \n",
    "\n",
    "Hierarchical clustering is an **unsupervised learning algorithm** that groups data points based on similarity. Unlike **K-Means clustering**, hierarchical clustering does not require specifying the number of clusters beforehand. Instead, it generates a **tree-like structure (dendrogram)** that visualizes the relationships between data points.  \n",
    "\n",
    "---\n",
    "\n",
    "### Workflow of Hierarchical Clustering  \n",
    "\n",
    "1. **Compute Distance Matrix**  \n",
    "   - Measure the similarity or distance between all pairs of data points using metrics like **Euclidean distance**.\n",
    "\n",
    "2. **Initialize Each Data Point as a Separate Cluster**  \n",
    "   - Begin with $ n $ clusters, where each cluster contains a single data point.\n",
    "\n",
    "3. **Merge the Closest Clusters**  \n",
    "   - Combine the two clusters that are the most similar.\n",
    "\n",
    "4. **Repeat Until One Cluster Remains**  \n",
    "   - Continue merging clusters iteratively until all points are grouped into a **single cluster**.\n",
    "\n",
    "---\n",
    "\n",
    "### Dendrogram: Visual Representation of Clusters  \n",
    "\n",
    "A **dendrogram** is a hierarchical tree structure that illustrates how clusters are formed.  \n",
    "- The **height of each merge** indicates the **dissimilarity** between clusters.  \n",
    "- A **horizontal cut** at a specific height determines the final number of clusters.  \n",
    "\n",
    "---\n",
    "\n",
    "### Types of Hierarchical Clustering  \n",
    "\n",
    "#### **1. Agglomerative Clustering (Bottom-Up)**  \n",
    "- Begins with **each data point as its own cluster**.  \n",
    "- Clusters are **iteratively merged** based on similarity.  \n",
    "- This is the **most common approach** to hierarchical clustering.  \n",
    "\n",
    "#### **2. Divisive Clustering (Top-Down)**  \n",
    "- Starts with **one large cluster** containing all data points.  \n",
    "- The cluster is **recursively split** into smaller clusters.  \n",
    "- Computationally more intensive and less commonly used.  \n",
    "\n",
    "---\n",
    "\n",
    "### Applications and Key Notes  \n",
    "- **Applications**: Gene expression analysis, market segmentation, and text analysis.  \n",
    "- **Advantages**: No need to predefine $ K $, and dendrograms provide a clear visualization of relationships.  \n",
    "- **Disadvantages**: Computationally expensive for large datasets.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Reference**  \n",
    "Batra, 2022  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model (GMM)\n",
    "\n",
    "The **Gaussian Mixture Model (GMM)** is a **probabilistic model** used for **unsupervised clustering**. Unlike **K-Means**, which assigns each data point to a single cluster, GMM is a **soft clustering method**, meaning each data point has a **probability of belonging to multiple clusters**.\n",
    "\n",
    "GMM assumes that data is generated from a mixture of multiple **Gaussian distributions**, where each distribution represents a **cluster** in the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "#### **How GMM Works**\n",
    "\n",
    "A **Gaussian Mixture Model** consists of **$K$ Gaussian distributions**, where each Gaussian is defined by three parameters:\n",
    "\n",
    "1. **Mean ($\\mu$)** – The center of the Gaussian distribution.\n",
    "2. **Covariance ($\\Sigma$)** – Defines the spread and shape of the data.\n",
    "3. **Mixing Coefficient ($\\pi$)** – The probability of each Gaussian component.\n",
    "\n",
    "Each data point is assigned a probability of belonging to a Gaussian component based on the **Gaussian probability density function (PDF)**:\n",
    "\n",
    "$$\n",
    "p(x | \\mu, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp \\left( -\\frac{1}{2} (x - \\mu)^T \\Sigma^{-1} (x - \\mu) \\right)\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $x$ is a data point,\n",
    "- $d$ is the number of dimensions,\n",
    "- $\\mu$ is the mean vector,\n",
    "- $\\Sigma$ is the covariance matrix.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Expectation-Maximization (EM) Algorithm**\n",
    "\n",
    "The **Expectation-Maximization (EM) algorithm** is an iterative optimization method used to find **maximum-likelihood estimates** for model parameters when the data is **incomplete** or contains **hidden variables**. EM helps estimate missing data points and refines model parameters iteratively until convergence.\n",
    "\n",
    "##### **Step 1: Expectation (E-Step)**  \n",
    "- Initialize the **model parameters**:  \n",
    "  - Mean ($\\mu_k$)  \n",
    "  - Covariance matrix ($\\Sigma_k$)  \n",
    "  - Mixing coefficients ($\\pi_k$)  \n",
    "- For each data point, calculate the **posterior probabilities** (i.e., the probability that a point belongs to a specific Gaussian component), represented by the **latent variables $\\gamma_k$**.\n",
    "\n",
    "##### **Step 2: Maximization (M-Step)**  \n",
    "- Update the **parameters** using the computed probabilities:\n",
    "  - **Mean ($\\mu_k$)**: Update using the **weighted average** of data points.\n",
    "  - **Covariance Matrix ($\\Sigma_k$)**: Update using the **weighted variance**.\n",
    "  - **Mixing Coefficients ($\\pi_k$)**: Update using the **average of latent probabilities**.\n",
    "\n",
    "##### **Step 3: Iterate Until Convergence**  \n",
    "- Repeat the **E-step and M-step** iteratively until:\n",
    "  - The **log-likelihood function stabilizes**.\n",
    "  - The **parameter updates become minimal**.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Reference**\n",
    "- (Carrasco, 2024)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why K-Means is the Best Choice\n",
    "\n",
    "##### **1. Computational Efficiency**\n",
    "- K-Means is significantly **faster** than hierarchical clustering and GMM.  \n",
    "- It operates in **$O(n \\times k \\times t)$** time complexity, where:  \n",
    "  - $n$ = number of data points  \n",
    "  - $k$ = number of clusters  \n",
    "  - $t$ = number of iterations  \n",
    "\n",
    "##### **2. Scalability**\n",
    "- Performs **efficiently on large datasets**, making it ideal for clustering **music genres** with **hundreds or thousands** of feature vectors.\n",
    "\n",
    "##### **3. Interpretability & Simplicity**\n",
    "- The **Elbow Method** helps determine the optimal number of clusters.  \n",
    "- Cluster assignments are **clear and deterministic**, ensuring each data point belongs to **only one cluster**.\n",
    "\n",
    "##### **4. Suitable for Music Genre Classification**\n",
    "- While GMM provides **probabilistic assignments**, K-Means is **better suited for distinct genre separation**.  \n",
    "- Most **music features** are naturally **separable**, making **hard clustering** an effective and efficient choice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although we decided to use KMeans, we still prove the existence of clusters using a Dendogram (Hierarchical Clustering). We use hierachical clustering to quickly show this because in dendograms are useful tools to visualise clusters and determine if there actaully appropriate clusters that can be worked with (Wilson, n.d.). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_cols = [col for col in labeled_features_df.columns if col not in ['filename']]\n",
    "genre_agg = labeled_features_df.groupby('genre')[group_cols].mean().reset_index()\n",
    "\n",
    "display(genre_agg)\n",
    "\n",
    "feature_columns = [col for col in genre_agg.columns if col != 'genre']\n",
    "X = genre_agg[feature_columns].values\n",
    "\n",
    "Z = linkage(X, method='ward')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "labels = labeled_features_df['genre'].values\n",
    "\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=genre_agg['genre'].values,\n",
    "    leaf_rotation=90,  \n",
    "    leaf_font_size=10,\n",
    "    color_threshold=0.7 * max(Z[:, 2]) \n",
    ")\n",
    "\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample (Genre)')\n",
    "plt.ylabel('Distance')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dendogram clearly shows that clusters exist. Based on the structure of the dendogram, there are most likely three clusters in this dataset. \n",
    "\n",
    "1. Cluster: (Classical, Blues, Jazz)\n",
    "2. Cluster: (Pop, Country)\n",
    "3. Cluster (Disco, Hiphop, Reggae, Metal, Rock)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 KMeans "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical operations of KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is an explanation of KMeans using a small fictious dataset of four datapoints (A,B,C,D) and 2 features (Feature 1 and Feature 2). To calcualte the distances, the Euclidean distance will be used. For simplicity we assume that the number of clusters is 2. \n",
    "\n",
    "| Point | Feature 1 (x₁) | Feature 2 (x₂) |\n",
    "|-------|----------------|----------------|\n",
    "| A     | 1.0            | 2.0            |\n",
    "| B     | 2.0            | 3.0            |\n",
    "| C     | 6.0            | 7.0            |\n",
    "| D     | 7.0            | 8.0            |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first, two random points will be selected to act as the centroids for the clusters. In our case we we will select Point A for cluster 1 and Point D cluster 2. \n",
    "\n",
    "<br>\n",
    "\n",
    "Then, we will calculate the distance from each point to the centroids using the Euclidean distance (theory see above). \n",
    "\n",
    "Point A: The distance towards the first centroid is 0 since its the same point. Point A therefore belongs to cluster 1.\n",
    "\n",
    "Point B: \n",
    "The distance from Point B towards the first centroid can be calculated by: $ \\sqrt{(2 - 1)^2 + (3 - 2)^2} = 1.41 $\n",
    "\n",
    "The distance from Point B towards the second centroid can be calculated by: $ \\sqrt{(2 - 7)^2 + (3 - 8)^2} = 7.07 $\n",
    "\n",
    "**It is quite clear, that Point B belongs to Cluster 1.**\n",
    "\n",
    "Point C:\n",
    "The distance from Point C towards the first centroid can be calculated by: $ \\sqrt{(6 - 1)^2 + (7 - 2)^2} = 7.07 $\n",
    "\n",
    "The distance from Point C towards the second centroid can be calculated by: $ \\sqrt{(7 - 7)^2 + (7 - 8)^2} = 1 $\n",
    "\n",
    "**It is quite clear, that Point C belongs to Cluster 2.**\n",
    "\n",
    "Point D: The distance towards the second centroid is 0 since its the same point. Point D therefore belongs to cluster 2.\n",
    "\n",
    "<br>\n",
    "  \n",
    "After that, the centroids of the newly formed clusters are calculated again (mean of all datapoints):\n",
    "\n",
    "For Cluster 1: Point A + B = $ ((1, 2) + (2, 3))/2 = (1,5/2,5) $\n",
    "\n",
    "For Cluster 2: Point C + D = $ ((6, 7) + (7, 8))/2 = (6,5/7,5) $\n",
    "\n",
    "Based on the newly calculated centroids the calculations from step 1 for all points (A,B,C,D):\n",
    "\n",
    "Point A: \n",
    "The distance from Point A towards the first centroid can be calculated by: $ \\sqrt{(1 - 1,5)^2 + (2 - 2,5)^2} = 0.71 $  \n",
    "\n",
    "The distance from Point A towards the second centroid can be calculated by: $ \\sqrt{(1 - 6,5)^2 + (2 - 7,5)^2} = 7.78 $\n",
    "\n",
    "**It is quite clear, that Point A belongs to Cluster 1.**\n",
    "\n",
    "Point B: \n",
    "The distance from Point B towards the first centroid can be calculated by: $ \\sqrt{(2 - 1,5)^2 + (3 - 2,5)^2} = 0.71 $\n",
    "\n",
    "The distance from Point B towards the second centroid can be calculated by: $ \\sqrt{(2 - 6,5)^2 + (3 - 7,5)^2} = 6.36 $\n",
    "\n",
    "**It is quite clear, that Point B belongs to Cluster 1.**\n",
    "\n",
    "Point C:\n",
    "The distance from Point C towards the first centroid can be calculated by: $ \\sqrt{(6 - 1,5)^2 + (7 - 2,5)^2} = 6.36$\n",
    "\n",
    "The distance from Point C towards the second centroid can be calculated by: $ \\sqrt{(6 - 6,5)^2 + (7 - 7,5)^2} = 0.71 $\n",
    "\n",
    "**It is quite clear, that Point C belongs to Cluster 2.**\n",
    "\n",
    "Point D: \n",
    "The distance from Point D towards the first centroid can be calculated by: $ \\sqrt{(7 - 1,5)^2 + (8 - 2,5)^2} = 7.78 $\n",
    "\n",
    "The distance from Point D towards the second centroid can be calculated by: $ \\sqrt{(7 - 6,5)^2 + (8 - 7,5)^2} = 0.71 $\n",
    "\n",
    "**It is quite clear, that Point D belongs to Cluster 2.**\n",
    "\n",
    "<br>\n",
    "\n",
    "Based on the calculations, KMeans would put Point A and B into the first cluster and Point C and D into the second cluster (Sena, 2024)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Optimal Number of Clusters for K-Means  \n",
    "This cell uses the `KMeansClustering` class to identify the optimal number of clusters for the unlabeled dataset:  \n",
    "- An instance of the `KMeansClustering` class is initialized with the scaled unlabeled feature data (`unlabeled_scaled`) and the original `unlabeled_features_df`.  \n",
    "- The `finding_k` method is called with a range of cluster numbers (`[2, 11]`).  \n",
    "\n",
    "The range `[2, 11]` was selected based on domain knowledge:  \n",
    "- The labeled dataset contains 10 genres, making it impossible for the unlabeled dataset to contain more than 10 meaningful clusters.  \n",
    "- A minimum of 2 clusters was chosen because having only 1 cluster would indicate no clustering, defeating the purpose of the analysis.  \n",
    "\n",
    "This process helps determine the ideal number of clusters for the dataset using metrics like the elbow method or silhouette scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmc = f.KMeansClustering(unlabeled_scaled, unlabeled_features_df)\n",
    "\n",
    "kmc.finding_k([2, 11])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining the Optimal Number of Clusters\n",
    "\n",
    "An iterative approach was applied to determine the optimal number of clusters for the KMeans algorithm. Cluster values ranging from 2 to 10 were evaluated. For each cluster count:\n",
    "\n",
    "- A KMeans model was trained using the scaled features of the \"unlabeled\" dataset.\n",
    "- The **inertia score** (sum of squared distances of samples to their closest cluster center) was recorded to quantify the model's performance.\n",
    "\n",
    "The results were visualized using an **Elbow Method plot** to identify the ideal number of clusters, where the inertia score shows a significant decrease before plateauing (GeeksforGeeks, 2024).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Number of Clusters\n",
    "\n",
    "Based on the **Elbow Plot**, the optimal number of clusters was determined to be **three**. This conclusion is drawn from the point where the inertia score shows a significant decrease and begins to plateau, indicating diminishing returns for higher cluster counts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_df = kmc.create_kmeans(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Data for Clustering and Classification  \n",
    "This cell prepares the data for further analysis and classification:  \n",
    "- The `cluster` column from the clustered DataFrame (`clustered_df`) is added to the `unlabeled_knn` DataFrame.  \n",
    "- The unlabeled dataset is split into features (`unlabeled_X`) and labels (`unlabeled_y`), where the `cluster` column serves as the label.  \n",
    "- The `labeled_knn` DataFrame is assigned to `labeled_X` for use in comparison or model training.  \n",
    "\n",
    "These steps ensure that both labeled and unlabeled datasets are properly structured for clustering and classification tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_knn['cluster'] = clustered_df['cluster']\n",
    "\n",
    "unlabeled_X, unlabeled_y = unlabeled_knn.drop('cluster', axis=1), unlabeled_knn['cluster']\n",
    "labeled_X = labeled_knn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 Determining genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcv = f.PostClusteringVisualizations(clustered_df, labeled_features_df)\n",
    "\n",
    "pcv.scatter_plot('spectral_centroid', 'spectral_bandwidth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Analysis: Spectral Bandwidth vs. Spectral Centroid  \n",
    "\n",
    "From the analysis of **spectral bandwidth** and **spectral centroid**, distinct clustering patterns were identified. The mapping of clusters to genres is as follows:  \n",
    "\n",
    "| **Cluster** | **Genres** |  \n",
    "| --- | --- |  \n",
    "| 0 | Pop |  \n",
    "| 1 | Classical |  \n",
    "| 2 | Hip-Hop, Metal, Rock |  \n",
    "\n",
    "This mapping highlights that certain genres, such as **Pop**, form a clearly defined cluster due to unique spectral properties, while others, such as **Hip-Hop**, **Metal**, and **Rock**, share overlapping features, grouping them into a single cluster. These results align with the expectation that genres with similar acoustic characteristics often exhibit overlapping clusters.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcv.scatter_plot('spectral_centroid', 'zero_crossing_rate')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Analysis: Zero Crossing Rate vs. Spectral Centroid  \n",
    "\n",
    "Analyzing the plots of **zero crossing rate** and **spectral centroid** revealed the following genre distributions across clusters:  \n",
    "\n",
    "| **Cluster** | **Genres** |  \n",
    "| --- | --- |  \n",
    "| 0 | Pop |  \n",
    "| 1 | Classical |  \n",
    "| 2 | Hip-Hop, Metal |  \n",
    "\n",
    "#### Key Insights:  \n",
    "- **Cluster 0**: Strongly associated with the **Pop** genre due to its distinct audio features, making it a well-defined cluster.  \n",
    "- **Cluster 1**: Clearly aligned with the **Classical** genre, indicating its unique spectral properties.  \n",
    "- **Cluster 2**: Encompasses genres such as **Hip-Hop** and **Metal**, suggesting overlapping characteristics in terms of zero crossing rate and spectral centroid.  \n",
    "\n",
    "This clustering analysis establishes a clear mapping for genres with distinct audio profiles, while highlighting the need for additional features or techniques to better separate overlapping genres within Cluster 2.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcv.scatter_plot('spectral_centroid', 'contrast_mean_2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Clustering Analysis: Contrast Mean 2 vs. Spectral Centroid\n",
    "\n",
    "By analyzing the plots of **Contrast Mean 2** and **Spectral Centroid**, the following genre distributions across clusters were identified:\n",
    "\n",
    "| **Cluster** | **Genres** |\n",
    "| --- | --- |\n",
    "| 0 | Pop |\n",
    "| 1 | Classical | \n",
    "| 2 | Metal |\n",
    "\n",
    "#### Final Genre Assignments\n",
    "From all previous visualizations, we can conclude the following:\n",
    "- **Cluster 0**: Pop\n",
    "- **Cluster 1**: Classical\n",
    "- **Cluster 2**: Metal\n",
    "\n",
    "#### Next Steps\n",
    "To validate these assumptions, a **K-Nearest Neighbors (KNN)** model will be trained to predict the clusters in the labeled dataset. This will help confirm the accuracy of the clustering results and the genre assignments.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=3)\n",
    "knn.fit(unlabeled_X, unlabeled_y)\n",
    "\n",
    "predicted_labels = knn.predict(labeled_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labeled_features_df['cluster'] = predicted_labels\n",
    "\n",
    "clusters_genres = labeled_features_df.groupby(['cluster', 'genre']).size().reset_index().sort_values(by=['cluster', 0], ascending=False)\n",
    "\n",
    "cluster_0 = clusters_genres[clusters_genres['cluster'] == 0]\n",
    "cluster_1 = clusters_genres[clusters_genres['cluster'] == 1]\n",
    "cluster_2 = clusters_genres[clusters_genres['cluster'] == 2]\n",
    "\n",
    "display(cluster_0.head())\n",
    "display(cluster_1.head())\n",
    "display(cluster_2.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation Results: KNN Model\n",
    "\n",
    "After running the KNN model, we can confirm that both the visual analysis and the mathematical operations arrive at the same conclusion regarding genre assignments:\n",
    "\n",
    "| **Cluster** | **Genres** |\n",
    "| --- | --- |\n",
    "| 0 | Pop |\n",
    "| 1 | Classical | \n",
    "| 2 | Metal |\n",
    "\n",
    "This alignment between visual insights and predictive modeling validates the clustering results, confirming the accuracy of the genre assignments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 Mapping clusters to genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "c0_g = 'pop'\n",
    "c1_g = 'classical'\n",
    "c2_g = 'metal'\n",
    "\n",
    "cluster_genre_mapping = {0: c0_g, 1: c1_g, 2: c2_g}\n",
    "\n",
    "kmc.cluster_to_genre(cluster_genre_mapping)\n",
    "\n",
    "kmc.create_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcar = f.PCAReduction(unlabeled_scaled)\n",
    "\n",
    "pcar.find_n()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Principal Component Analysis (PCA) Feature Selection  \n",
    "\n",
    "From the PCA plot, it is evident that there is a significant decline in explained variance between the 0th and 1st components, as well as between the 1st and 2nd components. However, beyond the 2nd component, the decline in explained variance becomes negligible.  \n",
    "\n",
    "Based on this observation, we will select **2 PCA features** for further analysis.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features = pcar.reduction(2)\n",
    "pca_features_labeled = pcar.reduce_labeled(labeled_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features_df = pd.DataFrame(pca_features, columns=['PCA 1', 'PCA 2'])\n",
    "pca_features_labeled_df = pd.DataFrame(pca_features_labeled, columns=['PCA 1', 'PCA 2'])\n",
    "pca_features_labeled_df['genre'] = labeled_features_df['genre']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1 Clustering with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmc_pca = f.KMeansClustering(pca_features, unlabeled_features_df)\n",
    "\n",
    "clustered_df_pca = kmc_pca.create_kmeans(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features_df['cluster'] = clustered_df_pca['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcv = f.PostClusteringVisualizations(pca_features_df, pca_features_labeled_df)\n",
    "pcv.scatter_plot('PCA 1', 'PCA 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PCA Results  \n",
    "\n",
    "The PCA reduction has effectively created two features that distinctly separate the three clusters, as observed in the plot. This clear separation indicates that the two principal components are sufficient to capture the underlying structure of the data and distinguish between the clusters.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2 Determining genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(clustered_df_pca['cluster'], clustered_df['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Consistency  \n",
    "\n",
    "From the crosstab, we can observe that the clusters are identical, with no differences between them. Therefore, we will apply the same cluster-to-genre mapping as we did for the standard KMeans clustering approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmc_pca.cluster_to_genre(cluster_genre_mapping)\n",
    "\n",
    "kmc_pca.create_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.3 Theory PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA (principal component analysis) is a way of dimension reduction. This means that using PCA you can analyse the dimension of the datset and reduce these to make a dataset which is smaller, which would improve runtime and makes models less complicated. A downside of PCA is that the resulting dataset is hard to interpret (Jaadi, 2024).\n",
    "\n",
    "PCA divided over 5 steps:\n",
    "1. Scaling the dataset: before any analysis can be run on a dataset it must be scaled.\n",
    "2. Calculate the covariance matrix: since the PCA reduction uses variance to determine which components need to be kept it is important to calculate to covariance \n",
    "3. Calculate the eigenvectors and eigenvalues of the covariancematrix: the values for the newly computed components are the eigenvalues. Therefore it is important to calculate these.\n",
    "4. Sort the eigenvectors from high to low and than sort the eigenvalues in the same order.\n",
    "5. Filter to the amount of components chosen.\n",
    "- (Jaadi, 2024)\n",
    "\n",
    "When would you use PCA?\n",
    "1. When working with linear data: other techniques like t-SNE and UMAP are best suited for non-linear data.\n",
    "2. Computaion: PCA is very computationally efficient.\n",
    "3. Information preservation: PCA preserves the maximum amount of variance in the dataset, which means that information is preserved most efficiently.\n",
    "- (Ibm, 2024)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmfr = f.NMFReduction(unlabeled_scaled)\n",
    "df_nmf_unlabeled = nmfr.reduction()\n",
    "df_nmf_labeled = nmfr.reduce_labeled(labeled_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf_labeled['genre'] = labeled_features_df['genre']\n",
    "\n",
    "display(df_nmf_unlabeled.head())\n",
    "display(df_nmf_labeled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1 Clustering with NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmc_nmf = f.KMeansClustering(df_nmf_unlabeled, unlabeled_features_df)\n",
    "\n",
    "clustered_df_nmf = kmc_nmf.create_kmeans(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_nmf_unlabeled['cluster'] = clustered_df_nmf['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pcv = f.PostClusteringVisualizations(df_nmf_unlabeled, df_nmf_labeled)\n",
    "pcv.scatter_plot(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2 Determining genres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df_nmf_unlabeled['cluster'], clustered_df['cluster'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Consistency  \n",
    "\n",
    "From the crosstab, we can observe that the clusters are identical, with no differences between them. Therefore, we will apply the same cluster-to-genre mapping as we did for the standard KMeans clustering approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmc_pca.cluster_to_genre(cluster_genre_mapping)\n",
    "\n",
    "kmc_pca.create_submission()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.3 Theory NMF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NMF (non-negative matric factorization) is a method of factorizing a non-negative matrix. It is an unsupervised learning algorithm which reduces dimensionality. It is used for recommendation systems, text mining, and image analysis applications (Eunus, 2025). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### 1. Important Features for Clustering\n",
    "After conducting feature engineering and unsupervised learning, we observed that the following sound features were crucial for effective clustering of music genres:\n",
    "\n",
    "- **Spectral Centroid**\n",
    "- **Spectral Bandwidth**\n",
    "- **Zero-Crossing Rate (ZCR)**\n",
    "- **Mel-Frequency Cepstral Coefficients (MFCCs)**\n",
    "- **Chroma Features**\n",
    "- **Tempo (BPM)**\n",
    "- **Spectral Contrast**\n",
    "- **Tonnetz (Tonal Centroid Features)**\n",
    "\n",
    "These features collectively improved the accuracy of clustering and genre classification.\n",
    "\n",
    "### 2. Effect and Usefulness of Dimensionality Reduction\n",
    "We applied two dimensionality reduction techniques:\n",
    "\n",
    "- **Principal Component Analysis (PCA)**: Reduced the feature space while retaining the most variance in data. The results showed that PCA helped in visualizing genre clusters effectively and improved classification performance by reducing noise.\n",
    "- **Non-negative Matrix Factorization (NMF)**: Provided a parts-based representation of data, which was particularly useful for uncovering underlying patterns in frequency distributions.\n",
    "\n",
    "### 3. Additional Data for Better Recommendations\n",
    "To enhance the clustering and recommendation system, we identified additional data that could improve results:\n",
    "\n",
    "- **Lyrics Analysis**: Lyrics-based features could enhance genre classification by providing contextual meaning to songs.\n",
    "- **Instrumentation Tags**: Identifying dominant instruments in each song (e.g., guitar for rock, synthesizers for electronic) could refine clustering.\n",
    "- **User Listening History**: Incorporating listener preferences and interaction data would make recommendations more personalized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Sources\n",
    "\n",
    "- Batra, R. (2022, February 12). The math behind the K-Means and hierarchical clustering algorithm! Medium. https://medium.com/@rohit_batra/the-math-behind-the-k-means-and-hierarchical-clustering-algorithm-1d9a36a56c08\n",
    "\n",
    "- Carrasco, O. C. (2024, March 1). Gaussian mixture models explained - towards data science. Medium. https://towardsdatascience.com/gaussian-mixture-models-explained-6986aaf5a95 \n",
    "\n",
    "- Dharmaraj. (2022, January 29). The math behind K-Means Clustering - Dharmaraj - Medium. Medium. https://medium.com/@draj0718/the-math-behind-k-means-clustering-4aa85532085e \n",
    "\n",
    "- Harpale, V. K., & Bairagi, V. K. (2021). Seizure detection methods and analysis. In Elsevier eBooks (pp. 51–100). https://doi.org/10.1016/b978-0-32-391120-7.00008-6 \n",
    "\n",
    "- Ibm. (2024, December 19). PCA. Think. https://www.ibm.com/think/topics/principal-component-analysis \n",
    "\n",
    "- Jaadi, Z. (2024, February 23). Principal Component Analysis (PCA): A Step-by-Step Explanation. Built In. https://builtin.com/data-science/step-step-explanation-principal-component-analysis \n",
    "\n",
    "- Librosa (n.d.-a) Spectral Centroid. Librosa. Retrieved January 3rd, 2025 from https://librosa.org/doc/main/generated/librosa.feature.spectral_centroid.html\n",
    "\n",
    "- Librosa (n.d.-b) Spectral Bandwidth. Librosa. Retrieved January 3rd, 2025 from https://librosa.org/doc/main/generated/librosa.feature.spectral_bandwidth.html \n",
    "\n",
    "- Librosa (n.d.-c). Zero-crossing rate. Librosa. (n.d.). https://librosa.org/doc/main/generated/librosa.feature.zero_crossing_rate.html \n",
    "\n",
    "- Librosa (n.d.-k). Spectral Rolloff. Librosa. Retrieved January 6th, 2025 from https://librosa.org/doc/main/generated/librosa.feature.spectral_rolloff.html\n",
    "\n",
    "- Selecting the number of clusters with silhouette analysis on KMeans clustering. (n.d.). Scikit-learn. https://scikit-learn.org/1.5/auto_examples/cluster/plot_kmeans_silhouette_analysis.html\n",
    "\n",
    "- So, N. L., Edwards, J. A., & Woolley, S. M. (2019). Auditory selectivity for spectral contrast in cortical neurons and behavior. Journal of Neuroscience, 40(5), 1015–1027. https://doi.org/10.1523/jneurosci.1200-19.2019\n",
    "\n",
    "- Wikipedia. (2025, January 5). Tempo. Wikipedia, Retrieved, January 5th, 2025, from https://en.wikipedia.org/wiki/Tempo\n",
    "\n",
    "- Wikipedia contributors. (2024, October 28). Spectral centroid. Wikipedia. https://en.wikipedia.org/wiki/Spectral_centroid\n",
    "\n",
    "- ZalaRushirajsinh. (2023, November 4). The elbow method: finding the optimal number of clusters. Medium. https://medium.com/@zalarushirajsinh07/the-elbow-method-finding-the-optimal-number-of-clusters-d297f5aeb189\n",
    "\n",
    "- Sena, M. (2024, May 22). Mastering K-means clustering: Implement the K-Means algorithm from scratch with this step-by-step Python tutorial. Towards Data Science. Retrieved January, 25th, 2025, from https://towardsdatascience.com/mastering-k-means-clustering-065bc42637e4\n",
    "\n",
    "- GeeksforGeeks. (2024, November 2). Elbow-Methode für den optimalen k-Wert in KMeans. GeeksforGeeks. Retrieved January 10th, 2025, from https://www.geeksforgeeks.org/elbow-method-for-optimal-value-of-k-in-kmeans/\n",
    "\n",
    "- Wilson, B. (n.d.). Visualization with hierarchical clustering and t-SNE [Video]. DataCamp. Retrieved, January 20th, 2025, from https://campus.datacamp.com/courses/unsupervised-learning-in-python/visualization-with-hierarchical-clustering-and-t-sne?ex=1\n",
    "\n",
    "- Sable, A. (2021). Introduction to audio analysis and synthesis. Paperspace Blog. Retrieved January 6th, 2025, from https://blog.paperspace.com/introduction-to-audio-analysis-and-synthesis/\n",
    "\n",
    "- Sable, A. (2022). An Introduction to Audio Analysis and Processing: Music Analysis. Paperspace Blog. Retrieved January 6th, 2025, from https://blog.paperspace.com/audio-analysis-processing-maching-learning/\n",
    "\n",
    "- Wikipedia. (2024, November 14-a). Spectral Flatness. Wikipedia. Retrieved, January 5th, 2025 from https://en.wikipedia.org/wiki/Spectral_flatness\n",
    "\n",
    "- OpenAE (n.d.-a). Zero-crossing rate. OpenAE. Retrieved January 6th, 2025 from https://openae.io/features/latest/zero-crossing-rate/\n",
    "\n",
    "- OpenAE (n.d.-b). RMS. OpenAE. Retrieved January 6th, 2025 from https://openae.io/features/latest/rms/\n",
    "\n",
    "- OpenAE (n.d.-c). Spectral Rolloff. OpenAE. Retrieved January 6th, 2025 from https://openae.io/features/latest/spectral-rolloff/\n",
    "\n",
    "- Bäckström, T., Räsänen, O., Zewoudie, A., Pérez Zarazaga, P., Koivusalo, L., Das, S., Gómez Mellado, E., Bouafif Mansali, M., Ramos, D., Kadiri, S., Alku, P., & Vali, M. H. (2022).  *Introduction to Speech Processing* (2nd ed.). Retrieved Janaury 7th, 2025 from https://speechprocessingbook.aalto.fi (doi: https://doi.org/10.5281/zenodo.6821775)\n",
    "\n",
    "- Miraglia, D. (2024, January 18-a). What is RMS in audio? The absolute BEST beginner’s guide. Unison Audio. Retrieved Janaury 7th, 2025 from https://unison.audio/what-is-rms-in-audio/\n",
    "\n",
    "- Wikipedia. (2024, November 2-b). Tonnetz. Wikipedia. Retrieved, January 5th, 2025 from https://en.wikipedia.org/wiki/Tonnetz\n",
    "\n",
    "- Ellis, D. P. W. (2007, July 16). Beat tracking by dynamic programming. LabROSA, Columbia University. Retrieved, January 14th, 2025 from https://www.ee.columbia.edu/~dpwe/pubs/Ellis07-beattrack.pdf\n",
    "\n",
    "- Wikipedia. (2024, Decmber 15-d). RMS. Retrieved 10th January, 2025 from https://en.wikipedia.org/wiki/Root_mean_square\n",
    "\n",
    "- OmniCalculator. (2024, July 28). OmniCalculator. Retrieved 7th January, 2025 https://www.omnicalculator.com/other/bpm\n",
    "\n",
    "- Englmeier, D., Hubig, N., Goebl, S., & Bohm, C. (2015). Musical similarity analysis based on chroma features and text retrieval methods. University of Munich; Helmholtz Center Munich.  Retrieved January, 28th from https://www.medien.ifi.lmu.de/pubdb/publications/pub/englmeier2015btw/englmeier2015btw.pdf\n",
    "\n",
    "- Yuto (2024, April 21). Musical similarity analysis based on chroma features and text retrieval methods. Zenn. Retrieved, January 14th, 2025 https://zenn.dev/yuto_mo/articles/7413ca2ed4eb5f\n",
    "\n",
    "- Wikipedia. (2025, January 8-b). Arithmetic mean. Wikipedia. Retrieved, January 28th, 2025 from https://en.wikipedia.org/wiki/Arithmetic_mean\n",
    "\n",
    "- Wikipedia. (2024, November 10). Mel-frequency cepstrum. Wikipedia. Retrieved, January 28th, 2025 from https://en.wikipedia.org/wiki/Mel-frequency_cepstrum\n",
    "\n",
    "- Spectral features. (n.d.). Music Information Retrieval. Retrieved, January 28th, 2025 from https://musicinformationretrieval.com/spectral_features.html\n",
    "\n",
    "- Jakeli, N. (2023, April 25). Clustering audio features. The MCT Blog. Retrieved, January 28th, 2025 from https://mct-master.github.io/blog/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
