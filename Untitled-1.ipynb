{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove if you need to install \n",
    "\n",
    "%pip install numpy\n",
    "%pip install matplotlib\n",
    "%pip install pandas\n",
    "%pip install seaborn\n",
    "%pip install scikit-learn\n",
    "%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_stroke_data = pd.read_csv('Datasets/train.csv')\n",
    "test_data = pd.read_csv('Datasets/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_stroke_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_stroke_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing or duplicated data in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_stroke_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics of the numerical columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncleaned_stroke_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics of the boolean columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncleaned_stroke_data.describe(include='bool'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual overview of individual columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following histograms provide an short overview of the columns and their distrubtion as well as their frequency in the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in uncleaned_stroke_data.columns:\n",
    "    sns.histplot(uncleaned_stroke_data[s])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncleaned_stroke_data['stroke'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'stroke' only contains \"0\" and \"1\" (with 0 being the predominant variable) which is essential to correctly train the model to predict strokes. However, the dataset is imbalanced (far more people who havent had strokes) which wil lead to wrong predictions. Later on, the data will be manipulated in order to counter this issue."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irrelevant Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all information is necessarily relevant for the model in order to correctly predict strokes. In the following we will have a look at irrelevant information within the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#corr_matrix = uncleaned_stroke_data.corr()\n",
    "\n",
    "#plt.figure(figsize=(22,22))\n",
    "##sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "#plt.title('Correlation Matrix')\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sta_sca(sc, df, cols):\n",
    "    for i in cols:\n",
    "        df[i] = sc.fit_transform(df[[i]])\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X = uncleaned_stroke_data.drop('stroke', axis=1)\n",
    "y = uncleaned_stroke_data['stroke']\n",
    "\n",
    "sta_sca(sc, X, ['age', 'avg_glucose_level', 'bmi'])\n",
    "\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X, y)\n",
    "\n",
    "ridge_importance = ridge_model.coef_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X.columns, ridge_importance)\n",
    "plt.xlabel('Ridge Coefficient')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance with Ridge Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X.corr()\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt=\".2f\", cmap='coolwarm')\n",
    "plt.title(\"Correlation Matrix of Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior analysis allows us to drop the following columns: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO: We still have to agree on which to remove according to our analysis: \n",
    "\n",
    "\"Some rows and/or columns may not be relevant to machine learning. Clean up the data so\n",
    "that only relevant rows remain.\"\n",
    "\n",
    "idk if my feature selection with Ridge was correct? but based on that we could drop some columns  - also consider values like married_yes or married_no...there can always be one value dropped bcs if married_yes=1 the other one is 0 automatically, right? so no additional information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['id', 'ever_married_no', 'Residence_type_rural']\n",
    "\n",
    "cleaned_stroke_data=uncleaned_stroke_data.drop(col_to_drop, axis=1)\n",
    "test_data_cleaned=test_data.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the remaining columns and their respective types: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stroke_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining X_train and y_train based on the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cleaned_stroke_data.drop('stroke', axis=1)\n",
    "y_train = cleaned_stroke_data['stroke']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to imbalance of the non-stroke vs. stroke cases, we need to resample our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='minority', random_state=0)\n",
    "\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our cleaned data we can now start to train the models to predict strokes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest-Neighbor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('knn', KNeighborsClassifier())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "param_grid = {'knn__n_neighbors': np.arange(1, 21), 'knn__weights': ['uniform', 'distance'], 'knn__algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute']}\n",
    "knn_cv = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_cv.fit(X_train, y_train)\n",
    "print(\"Best parameters: \", knn_cv.best_params_)\n",
    "print(\"Best cross-validation score: \", knn_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1, weights='uniform', algorithm='brute')\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = knn.predict(test_data_cleaned)\n",
    "\n",
    "test_data_cleaned['stroke'] = knn_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_cleaned.to_csv('Datasets/Predictions/test_lr_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('lr', LogisticRegression())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "param_grid = {'lr__class_weight': ['balanced', None], 'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga', 'newton-cholesky']}\n",
    "lr_cv = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv.fit(X_train, y_train)\n",
    "print(\"Best parameters: \", lr_cv.best_params_)\n",
    "print(\"Best cross-validation score: \", lr_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced', solver='newton-cholesky')\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(test_data_cleaned)\n",
    "\n",
    "test_data_cleaned['stroke'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_cleaned.to_csv('Datasets/Predictions/test_lr_pred.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting the best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('dt', DecisionTreeClassifier())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "param_grid = {\n",
    "    'dt__class_weight': ['balanced', None],\n",
    "    'dt__criterion': ['gini', 'entropy'],\n",
    "    'dt__max_depth': [None, 10, 20, 30],\n",
    "    'dt__min_samples_split': [2, 10, 20],\n",
    "    'dt__min_samples_leaf': [1, 5, 10]\n",
    "}\n",
    "\n",
    "dt_cv = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_cv.fit(X_train, y_train)\n",
    "print(\"Best parameters: \", dt_cv.best_params_)\n",
    "print(\"Best cross-validation score: \", dt_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier(class_weight='None', solver='entropy', max_depth=30, min_samples_leaf=1,min_samples_split=10)\n",
    "dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_pred = dt.predict(test_data_cleaned)\n",
    "\n",
    "test_data_cleaned['stroke'] = dt_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_cleaned.to_csv('Datasets/Predictions/test_dt_pred.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
