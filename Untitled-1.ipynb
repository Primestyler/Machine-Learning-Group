{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove if you need to install \n",
    "\n",
    "#%pip install numpy\n",
    "#%pip install matplotlib\n",
    "#%pip install pandas\n",
    "#%pip install seaborn\n",
    "#%pip install scikit-learn\n",
    "#%pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression, Ridge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_stroke_data = pd.read_csv('voorspellen-van-hartinfarct/train.csv')\n",
    "test_data = pd.read_csv('voorspellen-van-hartinfarct/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_stroke_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_stroke_data.duplicated().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is no missing or duplicated data in the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uncleaned_stroke_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics of the numerical columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncleaned_stroke_data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic statistics of the boolean columns: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncleaned_stroke_data.describe(include='bool'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visual overview of individual columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following histograms provide an short overview of the columns and their distrubtion as well as their frequency in the training dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in uncleaned_stroke_data.columns:\n",
    "    sns.histplot(uncleaned_stroke_data[s])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(uncleaned_stroke_data['stroke'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column 'stroke' only contains \"0\" and \"1\" (with 0 being the predominant variable) which is essential to correctly train the model to predict strokes in this classification problem - so no adjustment is needed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Irrelevant Information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not all information is necessarily relevant for the model in order to correctly predict strokes. In the following we will have a look at irrelevant information within the training dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = uncleaned_stroke_data.corr()\n",
    "\n",
    "plt.figure(figsize=(22,22))\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title('Correlation Matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sta_sca(sc, df, cols):\n",
    "    for i in cols:\n",
    "        df[i] = sc.fit_transform(df[[i]])\n",
    "\n",
    "sc = StandardScaler()\n",
    "\n",
    "X = uncleaned_stroke_data.drop('stroke', axis=1)\n",
    "y = uncleaned_stroke_data['stroke']\n",
    "\n",
    "sta_sca(sc, X, ['age', 'avg_glucose_level'])\n",
    "\n",
    "\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "ridge_model.fit(X, y)\n",
    "\n",
    "ridge_importance = ridge_model.coef_\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(X.columns, ridge_importance)\n",
    "plt.xlabel('Ridge Coefficient')\n",
    "plt.ylabel('Features')\n",
    "plt.title('Feature Importance with Ridge Regression')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ridge_importance.plot(kind='bar', figsize=(15, 8), color='skyblue')\n",
    "#plt.title('Feature Importance (Logistic Regression Coefficients)')\n",
    "#plt.ylabel('Coefficient Value')\n",
    "#plt.xlabel('Feature')\n",
    "#p#lt.xticks(rotation=45)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prior analysis allows us to drop the following columns: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TO-DO: We still have to agree on which to remove according to our analysis: \n",
    "\n",
    "\"Some rows and/or columns may not be relevant to machine learning. Clean up the data so\n",
    "that only relevant rows remain.\"\n",
    "\n",
    "idk if my feature selection with Ridge was correct? but based on that we could drop some columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "col_to_drop = ['id']\n",
    "\n",
    "cleaned_stroke_data=uncleaned_stroke_data.drop(col_to_drop, axis=1)\n",
    "test_data_cleaned=test_data.drop(col_to_drop, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overview of the remaining columns and their respective types: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_stroke_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining X_train and y_train based on the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = cleaned_stroke_data.drop('stroke', axis=1)\n",
    "y_train = cleaned_stroke_data['stroke']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to imbalance of the non-stroke vs. stroke cases, we need to resample our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(sampling_strategy='minority', random_state=0)\n",
    "\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using our cleaned data we can now start to train the models to predict strokes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting best hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps = [('lr', LogisticRegression())]\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "param_grid = {'lr__class_weight': ['balanced', None], 'lr__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga', 'newton-cholesky']}\n",
    "lr_cv = GridSearchCV(pipeline, param_grid, cv=5, scoring='f1', n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_cv.fit(X_train, y_train)\n",
    "print(\"Best parameters: \", lr_cv.best_params_)\n",
    "print(\"Best cross-validation score: \", lr_cv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression(class_weight='balanced', solver='newton-cholesky')\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(test_data_cleaned)\n",
    "\n",
    "test_data_cleaned['stroke'] = y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Editing test_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_cleaned.to_csv('voorspellen-van-hartinfarct/test.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
